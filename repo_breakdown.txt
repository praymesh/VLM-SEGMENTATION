CAT-Seg + Detectron2 + SIGLIP2 Pipeline Breakdown
====================================================

1. How does the model understand which class is assigned to which mask?
-----------------------------------------------------------------------
- Grayscale mask images: Each pixel value in the mask PNG corresponds to a class ID (e.g., pixel value 17 = "cat").
- Class mapping: The mapping from class ID to class name comes from the COCO_CATEGORIES list in cat_seg/data/datasets/register_coco_stuff.py. For example:
  {"id": 17, "name": "cat", ...}
- Meta info: The function _get_coco_stuff_meta() creates:
  - stuff_dataset_id_to_contiguous_id: Maps dataset class IDs to contiguous indices (for model output).
  - stuff_classes: List of class names in order.
- Detectron2's loader: When loading a mask, Detectron2 reads the pixel values and uses this mapping to know which class each pixel belongs to.

2. What happens after the data/images get loaded?
-------------------------------------------------
A. Dataset Registration and Loading
- In cat_seg/data/datasets/register_coco_stuff.py, the function register_all_coco_stuff_10k() registers your dataset with Detectron2.
- When you start training, Detectron2 uses this registration to load images and masks as pairs.

B. Data Mapping
- The file mask_former_semantic_dataset_mapper.py (class MaskFormerSemanticDatasetMapper) is responsible for:
  - Reading the image and mask.
  - Applying augmentations and resizing.
  - Converting the image to a tensor (C, H, W).
  - Converting the mask to a tensor (H, W) with class indices.
  - Returning a dictionary with keys "image" and "sem_seg".

3. How are images and masks processed in the model?
---------------------------------------------------
A. Model Input
- The main model class is CATSeg.
- In the forward() method, it receives a batch of dictionaries, each with:
  - "image": Tensor of the image (C, H, W).
  - "sem_seg": Tensor of the mask (H, W) with class indices.

B. Preprocessing
- The image is normalized using the mean and std (for both backbone and SIGLIP2).
- The image is resized to the SIGLIP2 input resolution (e.g., 224x224, 384x384).

C. Feature Extraction
- The normalized and resized image is passed to the SIGLIP2 processor and model:
  inputs = self.sem_seg_head.predictor.processor(images=clip_images_resized, return_tensors="pt").to(self.device)
  clip_features = self.sem_seg_head.predictor.clip_model.get_image_features(**inputs)
- This produces image features (embeddings).

D. Patch/Token Processing
- For ViT-based models (like SIGLIP2), the image is split into patches inside the model (not in your code).
- The output is a sequence of patch embeddings.

E. Feature Rearrangement
- The patch embeddings are rearranged into spatial feature maps using einops.rearrange for further processing.

F. Segmentation Head
- The features are passed to the segmentation head (sem_seg_head), which combines them with text features (class embeddings) and produces per-pixel class logits.

G. Loss or Output
- During training: The output is compared to the ground-truth mask using a loss function (e.g., binary cross-entropy).
- During inference: The output is post-processed to produce the final segmentation mask.

4. Which files/functions handle each step?
------------------------------------------
| Step                        | File/Function                                                                                   |
|-----------------------------|-----------------------------------------------------------------------------------------------|
| Dataset registration        | cat_seg/data/datasets/register_coco_stuff.py: register_all_coco_stuff_10k()           |
| Data loading/mapping        | mask_former_semantic_dataset_mapper.py: __call__ |
| Model entry point           | cat_seg/cat_seg_model.py: CATSeg.forward()                                    |
| Image normalization/resizing| cat_seg/cat_seg_model.py: forward()                                           |
| SIGLIP2 feature extraction  | cat_seg/cat_seg_model.py: forward() (calls processor/model)                   |
| Patch embedding (ViT)       | Inside SIGLIP2 model (Hugging Face Transformers)                                              |
| Feature rearrangement       | cat_seg/cat_seg_model.py: rearrange()                                         |
| Segmentation head           | sem_seg_head: called in forward()                                     |
| Loss/postprocessing         | cat_seg/cat_seg_model.py: forward()                                           |

5. Detailed Flow Example
------------------------
1. Image/mask loaded by Detectron2 using the registered dataset.
2. Mapper (MaskFormerSemanticDatasetMapper) processes and returns a dict:
   - "image": Tensor (C, H, W)
   - "sem_seg": Tensor (H, W) with class indices
3. Model forward:
   - Normalizes and resizes image for SIGLIP2.
   - Passes image through SIGLIP2 processor/model to get features.
   - Rearranges features for segmentation.
   - Passes features and text embeddings to segmentation head.
   - Computes loss (training) or outputs mask (inference).

6. Summary
----------
- Class mapping is handled by the COCO_CATEGORIES and meta info in cat_seg/data/datasets/register_coco_stuff.py.
- Image/mask loading and conversion is handled by the dataset mapper.
- Model input is always tensors: images as (C, H, W), masks as (H, W) with class indices.
- SIGLIP2 handles patchification internally.
- Segmentation head combines image and text features for per-pixel classification.

7. Trace a Specific Image Through the Pipeline
---------------------------------------------
Suppose you have an image file: datasets/coco-stuff/images/val2017/000000000001.jpg and its mask: datasets/coco-stuff/annotations_detectron2/val2017/000000000001.png

Step-by-step trace:
1. **Dataset Registration**: The dataset is registered via register_all_coco_stuff_10k().
2. **DataLoader**: During training or evaluation, Detectron2's DataLoader picks the image/mask pair.
3. **Dataset Mapper**: MaskFormerSemanticDatasetMapper.__call__ is invoked:
   - Reads the image and mask from disk.
   - Applies augmentations (resize, crop, flip, color, etc.).
   - Converts image to tensor (C, H, W), mask to tensor (H, W) with class indices.
   - Pads image/mask if needed.
   - Returns a dict: {"image": image_tensor, "sem_seg": mask_tensor}
4. **Model Forward**: CATSeg.forward() is called with the batch dict:
   - Receives the image and mask tensors.
   - Normalizes and resizes the image for SIGLIP2.
   - Prepares the image for the SIGLIP2 processor/model.
   - Calls self.sem_seg_head.predictor.processor(images=img, ...) and self.sem_seg_head.predictor.clip_model.get_image_features(**inputs) to get image features.
   - Rearranges features for segmentation.
   - Gets text features (class embeddings) using the predictor and SIGLIP2.
   - Passes features to the segmentation head.
   - During training: computes loss with the ground-truth mask.
   - During inference: outputs the predicted segmentation mask.
5. **Output**: The output is either a loss (for backprop) or a segmentation mask (for evaluation/inference).

8. Tensor-Level Trace: Shapes and Transformations
------------------------------------------------
Let's trace a single image (e.g., 000000000001.jpg) and its mask through the pipeline, focusing on tensor shapes at each step. Assume the original image is 480x640 (HxW), and SIGLIP2 expects 224x224.

1. **Dataset Mapper (MaskFormerSemanticDatasetMapper.__call__)**
   - Reads image: shape (480, 640, 3) [H, W, C], dtype uint8
   - Reads mask: shape (480, 640), dtype uint8 or int32
   - Applies augmentations (resize, crop, flip, etc.): image and mask may change size
   - Converts image to tensor: shape (3, H, W), dtype torch.float32 (after transpose and conversion)
   - Converts mask to tensor: shape (H, W), dtype torch.int64
   - Pads image/mask if needed (to size_divisibility): e.g., (3, 480, 640) → (3, 480, 640)
   - Returns dict: {"image": (3, H, W), "sem_seg": (H, W)}

2. **Model Forward (CATSeg.forward)**
   - Receives batch: [dict with "image": (3, H, W), "sem_seg": (H, W)]
   - Normalizes image: (3, H, W), dtype torch.float32
   - Resizes image for SIGLIP2: (3, H, W) → (3, 224, 224) (using interpolation)
   - Converts to PIL or numpy for processor if needed

3. **SIGLIP2 Processor**
   - Input: image (3, 224, 224), dtype float32 or PIL
   - Output: pixel_values: (1, 3, 224, 224), dtype torch.float32 (batch dimension added)

4. **SIGLIP2 Model (get_image_features)**
   - Input: pixel_values: (1, 3, 224, 224)
   - Internally, splits into patches: (1, 3, 224, 224) → (1, N_patches, patch_dim)
     - For ViT-base patch16: N_patches = (224/16)*(224/16) = 14*14 = 196
     - patch_dim = 768 (for base), 1024/1280 for larger models
   - Output: image_embeds: (1, D), e.g., (1, 768) or (1, 1024) depending on model

5. **Feature Rearrangement (einops.rearrange)**
   - If needed, can reshape patch embeddings to (B, C, H', W') for segmentation head
   - Example: (1, 196, 768) → (1, 768, 14, 14)

6. **Text Embeddings (SIGLIP2 get_text_features)**
   - Input: tokenized text (batch_size, seq_len), e.g., (N_classes, L)
   - Output: text_embeds: (N_classes, D) (D matches image_embeds dim)

7. **Segmentation Head**
   - Receives: image features (B, C, H', W'), text features (N_classes, C)
   - Produces: per-pixel logits (B, N_classes, H', W')

8. **Loss or Output**
   - During training: compares (B, N_classes, H', W') with ground-truth mask (B, H', W')
   - During inference: outputs (B, H', W') with predicted class per pixel

**Example for a single image:**
- Input image: (3, 480, 640)
- After resize: (3, 224, 224)
- After processor: (1, 3, 224, 224)
- After SIGLIP2: (1, 768) or (1, 1024)
- After rearrange: (1, 768, 14, 14)
- Text features: (N_classes, 768)
- Segmentation logits: (1, N_classes, 14, 14)
- Final mask: (1, 14, 14) (can be upsampled to original size)

This gives you a full tensor-level trace of the data as it flows through your pipeline.
